{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrl7012005/TF-NLP/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PY7OW3ivQ5gd"
      },
      "source": [
        "We will use a transformer. The decoder is built from attention layers using self attention to process the sequence being generated. It uses cross attention to attend to the image. By looking at the cross attention weights, we can see what part of the image is being tended to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWvco72MQvrz",
        "outputId": "4fd642aa-6d1a-4c7f-e594-72ba52324a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_text\n",
            "  Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.10/dist-packages (4.9.6)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_text) (2.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (8.1.7)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.8)\n",
            "Requirement already satisfied: immutabledict in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.26.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (5.9.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (14.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.32.3)\n",
            "Requirement already satisfied: simple-parsing in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.1.6)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (2.4.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (4.66.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (1.16.0)\n",
            "Requirement already satisfied: array-record>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow_datasets) (0.5.1)\n",
            "Requirement already satisfied: etils>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (1.9.4)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (4.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow_datasets) (3.20.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2024.8.30)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (24.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow_text) (0.37.1)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.10/dist-packages (from simple-parsing->tensorflow_datasets) (0.16)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (0.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow_text) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow_text) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow_text) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow_text) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow_text) (0.1.2)\n",
            "Downloading tensorflow_text-2.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorflow_text\n",
            "Successfully installed tensorflow_text-2.17.0\n"
          ]
        }
      ],
      "source": [
        "!apt install --allow-change-held-packages libcudnn8=8.6.0.163-1+cuda11.8\n",
        "!pip uninstall -y tensorflow estimator keras\n",
        "!pip install -U tensorflow_text tensorflow tensorflow_datasets\n",
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39z1JhRDSkyw"
      },
      "outputs": [],
      "source": [
        "import concurrent.futures\n",
        "import collections\n",
        "import dataclasses\n",
        "import hashlib\n",
        "import itertools\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pathlib\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import time\n",
        "import urllib.request\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import requests\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdBitqg4Stwx"
      },
      "source": [
        "#Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28-z668dS367"
      },
      "source": [
        "Datasets are available at Flick8r, or the full Conceptual Captions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jsd61CBSvCx"
      },
      "outputs": [],
      "source": [
        "def flick8r(path='flickr8k'):\n",
        "  path = pathlib.Path(path)\n",
        "\n",
        "  if len(list(path.rglob('*'))) < 16197:\n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "\n",
        "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "  captions = (line.split('\\t') for line in captions)\n",
        "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "  cap_dict = collections.defaultdict(list)\n",
        "  for fname, cap in captions:\n",
        "    cap_dict[fname].append(cap)\n",
        "\n",
        "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "  train_ds = tf.data.experimental.from_list(train_captions)\n",
        "  test_ds = tf.data.experimental.from_list(test_captions)\n",
        "\n",
        "  return train_ds, test_ds\n",
        "\n",
        "def conceptual_captions(*, data_dir=\"conceptual_captions\", num_train, num_val):\n",
        "  def iter_index(index_path):\n",
        "    with open(index_path) as f:\n",
        "      for line in f:\n",
        "        caption, url = line.strip().split('\\t')\n",
        "        yield caption, url\n",
        "\n",
        "  def download_image_urls(data_dir, urls):\n",
        "    ex = concurrent.futures.ThreadPoolExecutor(max_workers=100)\n",
        "    def save_image(url):\n",
        "      hash = hashlib.sha1(url.encode())\n",
        "      # Name the files after the hash of the URL.\n",
        "      file_path = data_dir/f'{hash.hexdigest()}.jpeg'\n",
        "      if file_path.exists():\n",
        "        # Only download each file once.\n",
        "        return file_path\n",
        "\n",
        "      try:\n",
        "        result = requests.get(url, timeout=5)\n",
        "      except Exception:\n",
        "        file_path = None\n",
        "      else:\n",
        "        file_path.write_bytes(result.content)\n",
        "      return file_path\n",
        "\n",
        "    result = []\n",
        "    out_paths = ex.map(save_image, urls)\n",
        "    for file_path in tqdm.tqdm(out_paths, total=len(urls)):\n",
        "      result.append(file_path)\n",
        "\n",
        "    return result\n",
        "\n",
        "  def ds_from_index_file(index_path, data_dir, count):\n",
        "    data_dir.mkdir(exist_ok=True)\n",
        "    index = list(itertools.islice(iter_index(index_path), count))\n",
        "    captions = [caption for caption, url in index]\n",
        "    urls = [url for caption, url in index]\n",
        "\n",
        "    paths = download_image_urls(data_dir, urls)\n",
        "\n",
        "    new_captions = []\n",
        "    new_paths = []\n",
        "    for cap, path in zip(captions, paths):\n",
        "      if path is None:\n",
        "        # Download failed, so skip this pair.\n",
        "        continue\n",
        "      new_captions.append(cap)\n",
        "      new_paths.append(path)\n",
        "\n",
        "    new_paths = [str(p) for p in new_paths]\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((new_paths, new_captions))\n",
        "    ds = ds.map(lambda path,cap: (path, cap[tf.newaxis])) # 1 caption per image\n",
        "    return ds\n",
        "\n",
        "  data_dir = pathlib.Path(data_dir)\n",
        "  train_index_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/gcc-data/Train/GCC-training.tsv',\n",
        "    cache_subdir=data_dir,\n",
        "    cache_dir='.')\n",
        "\n",
        "  val_index_path = tf.keras.utils.get_file(\n",
        "    origin='https://storage.googleapis.com/gcc-data/Validation/GCC-1.1.0-Validation.tsv',\n",
        "    cache_subdir=data_dir,\n",
        "    cache_dir='.')\n",
        "\n",
        "  train_raw = ds_from_index_file(train_index_path, data_dir=data_dir/'train', count=num_train)\n",
        "  test_raw = ds_from_index_file(val_index_path, data_dir=data_dir/'val', count=num_val)\n",
        "\n",
        "  return train_raw, test_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhLnHYDJTK7u"
      },
      "source": [
        "Use flicker first as it contains 5 captions per image and isnt insanely large"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gGV9m28TQW9",
        "outputId": "c35694c9-5301-4f8c-f697-16044eb50660"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
            "\u001b[1m1115419746/1115419746\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 0us/step\n",
            "Downloading data from https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\n",
            "\u001b[1m2340801/2340801\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "choose = 'flickr8k'\n",
        "\n",
        "if choose == 'flickr8k':\n",
        "  train_raw, test_raw = flick8r()\n",
        "else:\n",
        "  train_raw, test_raw = conceptual_captions(num_train=10000, num_val=5000)\n",
        "\n",
        "train_raw.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgXev6PeToZG"
      },
      "source": [
        "Each element of the dataset is the image path and image caption rather than the actual image as that saves more space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns7QSNDhTh9I",
        "outputId": "e18a15ea-0267-4ed5-a1ca-8b00b241189e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(b'flickr8k/Flicker8k_Dataset/2513260012_03d33305cf.jpg', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[b'A black dog is running after a white dog in the snow .'\n",
            " b'Black dog chasing brown dog through snow'\n",
            " b'Two dogs chase each other across the snowy ground .'\n",
            " b'Two dogs play together in the snow .'\n",
            " b'Two dogs running through a low lying body of water .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for ex_path, ex_captions in train_raw.take(1):\n",
        "  print(ex_path)\n",
        "  print(ex_captions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKfCbcXQT73r"
      },
      "source": [
        "For out feature extractor from the image, we will use MobileNet pretrained on ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P64eMLW3UVGF",
        "outputId": "59cb036c-b2e7-4cdd-9bee-b0795f84a880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v3/weights_mobilenet_v3_small_224_1.0_float_no_top_v2.h5\n",
            "\u001b[1m4334752/4334752\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True)\n",
        "mobilenet.trainable=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rZBiKvUUWNb"
      },
      "source": [
        "We can create a function to load the image from a path and resize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrgyqD1XUas-",
        "outputId": "feefbd54-6e80-4095-d286-e7109476ebd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1, 224, 224, 3)\n",
            "(1, 7, 7, 576)\n"
          ]
        }
      ],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img\n",
        "\n",
        "test_img_batch = load_image(ex_path)[tf.newaxis, :]\n",
        "print(test_img_batch.shape)\n",
        "print(mobilenet(test_img_batch).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64q18_BfUqo3"
      },
      "source": [
        "Now lets setup the tokenizer and vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcemqcldUs3a",
        "outputId": "89e25417-45ca-4585-a94b-1f191bb27621"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['', '[UNK]', 'a', '[START]', '[END]', 'in', 'the', 'on', 'is', 'and']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]','')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s\n",
        "\n",
        "vocabulary_size = 5000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=standardize,\n",
        "    max_tokens=vocabulary_size,\n",
        "    ragged=True\n",
        ")\n",
        "\n",
        "tokenizer.adapt(train_raw.map(lambda fp, txt: txt).unbatch().batch(1024))\n",
        "\n",
        "tokenizer.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nLF06LCXVw24",
        "outputId": "c9dc0921-ccf0-4756-d205-284c3a9301c4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[3, 2, 655, 5, 2, 97, 4], [3, 2, 1937, 10, 4]]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t = tokenizer([['a cat in a hat'], ['a robot dog']])\n",
        "t"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rZiBib9V13i"
      },
      "source": [
        "Create mapping for words to indices and indices to words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wa4O7t-V5Pz"
      },
      "outputs": [],
      "source": [
        "word_to_index = tf.keras.layers.StringLookup(mask_token = \"\",\n",
        "                                             vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(mask_token = \"\",\n",
        "                                             vocabulary=tokenizer.get_vocabulary(),\n",
        "                                             invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXbkg2ISWXB4",
        "outputId": "abda9863-6f8b-4af4-c219-d13117b1e543"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[b'[START]', b'a', b'cat', b'in', b'a', b'hat', b'[END]'],\n",
              " [b'[START]', b'a', b'robot', b'dog', b'[END]']]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "w = index_to_word(t)\n",
        "w.to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rCmNF73WZ_1",
        "outputId": "0d67b433-06c6-4cb7-b5f3-b9b1a1b89737"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'[START] a cat in a hat [END]', b'[START] a robot dog [END]'],\n",
              "      dtype=object)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.strings.reduce_join(w, separator=' ', axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ApobBlplVvh4"
      },
      "source": [
        "#Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpZj3xEAWifO"
      },
      "source": [
        "The current set contains many captions per image, lets replicate the image so we have many 1:1 image captions instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_blKPHw5WpEI"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xGb5nRsWtlI",
        "outputId": "83404c4e-96a6-43a1-9ec1-e9b0c5db1cb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBjyHTUGX_f4"
      },
      "source": [
        "For text generation datasets should contain an input and the labels shifted by one step. We need to convert (images, texts) pair to an ((images, input_tokens), label_tokens) pair."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8vV9S2gYPMq"
      },
      "outputs": [],
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokenizer(txts)[..., :-1]\n",
        "  label_tokens = tokenizer(txts)[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfYNb_yPYa9s"
      },
      "source": [
        "Lets combine everything to make our pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gt5yLjMYjz2"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "\n",
        "  ds = (ds.shuffle(10000).map(lambda path, caption:(load_image(path), caption))\n",
        "    .apply(tf.data.experimental.ignore_errors()).batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds.map(match_shapes, tf.data.AUTOTUNE).unbatch().shuffle(shuffle_buffer)\n",
        "  .batch(batch_size).map(prepare_txt, tf.data.AUTOTUNE).map(to_tensor, tf.data.AUTOTUNE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjH1jhs0ZGjN",
        "outputId": "d7fe9acc-2e89-49c0-c97d-6cf5b00bf45a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-17-aa67e807be7f>:4: ignore_errors (from tensorflow.python.data.experimental.ops.error_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.ignore_errors` instead.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "   TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " ((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "   TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "\n",
        "train_ds.element_spec, test_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjI3camiZRqp"
      },
      "source": [
        "Now cache the image features since the image feature extractor is not changing, and no augmentation can be used. Same for the text tokenization. It will improve the function of our program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4KLt3iUZbEP"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
        "\n",
        "  ds = (ds.map(lambda path, caption: (load_image(path), caption))\n",
        "  .apply(tf.data.experimental.ignore_errors()).batch(batch_size))\n",
        "\n",
        "  #Run feature extractor on each batch and dont do this on .map as\n",
        "  #tf.data run on CPU\n",
        "  def gen():\n",
        "    for (images, captions) in tqdm.tqdm(ds):\n",
        "      feature_maps = image_model(images)\n",
        "\n",
        "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
        "\n",
        "      #Instead of returning single val, returns multiple overtime\n",
        "      #Allowing for lazy eval where values are only computed when requested\n",
        "      yield feature_maps, captions\n",
        "\n",
        "  new_ds = tf.data.Dataset.from_generator(\n",
        "      gen,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=image_model.output_shape),\n",
        "          tf.TensorSpec(shape=(None,), dtype=tf.string))\n",
        "      )\n",
        "\n",
        "  new_ds = new_ds.map(prepare_txt, tf.data.AUTOTUNE).unbatch().shuffle(1000)\n",
        "\n",
        "  #Shard the dataset\n",
        "  def shard_func(i, item):\n",
        "    return i % shards\n",
        "\n",
        "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
        "\n",
        "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1000)\n",
        "\n",
        "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
        "\n",
        "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_index(i, x):\n",
        "    return x\n",
        "\n",
        "  ds = (ds.map(drop_index, tf.data.AUTOTUNE).shuffle(shuffle).padded_batch(batch_size).prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRglbrimatYP",
        "outputId": "07bcbcbf-fc0b-4eb0-d08f-748c1fdb5010"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "188it [01:23,  2.25it/s]\n",
            "32it [00:10,  3.07it/s]\n"
          ]
        }
      ],
      "source": [
        "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
        "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgj6xzEHbMQ5",
        "outputId": "d8169a16-313a-4706-b4dd-67192d2c2874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(32, 7, 7, 576)\n",
            "(32, 19)\n",
            "(32, 19)\n",
            "[  3   2  41  17 338  37   2 179  36 413   0   0   0   0   0   0   0   0\n",
            "   0]\n",
            "[  2  41  17 338  37   2 179  36 413   4   0   0   0   0   0   0   0   0\n",
            "   0]\n"
          ]
        }
      ],
      "source": [
        "train_ds = load_dataset('train_cache')\n",
        "test_ds = load_dataset('test_cache')\n",
        "\n",
        "for (inputs, ex_labels) in train_ds.take(1):\n",
        "  (ex_img, ex_in_tok) = inputs\n",
        "\n",
        "print(ex_img.shape)\n",
        "print(ex_in_tok.shape)\n",
        "print(ex_labels.shape)\n",
        "\n",
        "print(ex_in_tok[0].numpy())\n",
        "print(ex_labels[0].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6cy6dEZbYAl"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsS37Qq5bc8F"
      },
      "source": [
        "There is an input portion for the embeddings and encodings. There is a decoder containing masked self attention, cross attention and a feed forward to process each output location independently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jn7xsfub4aL"
      },
      "source": [
        "**Embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVTXgJqDbaBQ"
      },
      "outputs": [],
      "source": [
        "#We'll learn the positional embeddings, but this doesnt generalize to longer sequences\n",
        "\n",
        "class SeqEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, vocab_size, max_length, depth):\n",
        "    super().__init__()\n",
        "\n",
        "    self.pos_embedding = tf.keras.layers.Embedding(input_dim=max_length, output_dim=depth)\n",
        "    self.token_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=depth, mask_zero=True)\n",
        "\n",
        "    self.add = tf.keras.layers.Add()\n",
        "\n",
        "  def call(self, seq):\n",
        "\n",
        "    seq = self.token_embedding(seq) #(B, T, C)\n",
        "\n",
        "    x = tf.range(tf.shape(seq)[1])\n",
        "    x = x[tf.newaxis, :]\n",
        "    x = self.pos_embedding(x)\n",
        "\n",
        "    return self.add([seq, x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8QhMKAxcbDp"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x, use_causal_mask = True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n",
        "\n",
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add()\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(query=x, value=y,\n",
        "                                      return_attention_scores=True)\n",
        "\n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n",
        "\n",
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate)\n",
        "    ])\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beSlhYXRgTAX"
      },
      "source": [
        "Now build a decoder block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnKgRQbkgU7z"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "\n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "\n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVT10VWNgmGf"
      },
      "source": [
        "At minimum we need a Dense layer in the output to generate logit predictions for each token at each location, but we can also handle bad tokens. It should never generate a pad, unknown or start token or ''. So set the bias for these to a large negative value, make it very hard to activate.\n",
        "\n",
        "We can use a smart initialization. The default initialization gives each token with an apprixmate uniform likelihood, but the actual distribution is far ffrom uniform. The optimal initial bias of the output is log(prob each token) so include an adapt method. This reduces the initial loss from the entropy of the uniform distirbution log(vocab_size) to the marginal entropy of the distribution -p*log(p)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_A0HjY-hfl6"
      },
      "outputs": [],
      "source": [
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('','[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__()\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=tokenizer.vocabulary_size(), **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr == 0] = 1.0\n",
        "    log_p = np.log(p)\n",
        "\n",
        "    entropy = - (log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    #Add layer doesnt work as its different shapes, this clears the mask\n",
        "    return x + self.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGg-ABMmjJBg",
        "outputId": "8d82dd8e-bd26-4ee1-ebc1-2eee1f7dc65d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:07<00:00, 126.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uniform entropy: 8.52\n",
            "Marginal entropy: 5.29\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLT7By4-jwIi"
      },
      "source": [
        "Now lets create the captioner."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VIDwB4mjywF"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True)\n",
        "\n",
        "    self.seq_embedding = SeqEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        depth=units,\n",
        "        max_length=max_length)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLshEEB72gxv"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def call(self, inputs):\n",
        "  image, txt = inputs\n",
        "\n",
        "  if image.shape[-1] == 3:\n",
        "    image = self.feature_extractor(image)\n",
        "\n",
        "  image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "  if txt.dtype == tf.string:\n",
        "    txt = tokenizer(txt)\n",
        "\n",
        "  txt = self.seq_embedding(txt)\n",
        "\n",
        "  for dec_layer in self.decoder_layers:\n",
        "    txt = dec_layer(inputs=(image, txt))\n",
        "\n",
        "  txt = self.output_layer(txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoz-QPNuqGtc"
      },
      "outputs": [],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer, units=256, dropout_rate=0.5, num_layers=2, num_heads=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLJwYB3ArYLa"
      },
      "source": [
        "Lets write some code to generate captions to see how training is progressing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV1Zyx97sFJk",
        "outputId": "09b22202-6aca-4523-cbcc-f6c5cd93c0ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://tensorflow.org/images/surf.jpg\n",
            "\u001b[1m64400/64400\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n"
          ]
        }
      ],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKC9MVKBsUwf"
      },
      "source": [
        "To caption an image, extract the features. Intialize the list of output tokens with a [START] token and pass img_features and tokens into the model. It return a list of logits, chooses the next token based on those logits, add it to the list of tokens and continue the loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD3fBXpjsimH"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1):\n",
        "  initial = self.word_to_index([['[START]']]) #(B, T)\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial\n",
        "\n",
        "  for n in range(50):\n",
        "    preds = self((img_features, tokens)).numpy() #(B, T, C)\n",
        "    preds = preds[:, -1, :]\n",
        "\n",
        "    if temperature == 0:\n",
        "      next = tf.argmax(preds, axis=-1)[:, tf.newaxis] #(B, 1)\n",
        "    else:\n",
        "      next = tf.random.categorical(preds/temperature, num_samples=1) #(B, 1)\n",
        "\n",
        "    tokens = tf.concat([tokens, next], axis=1) #(B, T)\n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9OF5uPOtnSO",
        "outputId": "f7965985-cbf4-401a-f3df-5fd2d3c728fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a man in a white dog in the water\n",
            "a man in a man in a a of a man\n",
            "two brown boots two runs on the standing hat in the little older to laptop dogs in the kid\n"
          ]
        }
      ],
      "source": [
        "for t in (0.0, 0.5, 1.0):\n",
        "  result = model.simple_gen(image, temperature=t)\n",
        "  print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idtkWMRLtvzS"
      },
      "source": [
        "The temperature allows us to interpolate between 3 modes. temp = 0 chooses the most likely next token at each step, greedy decoding. temp = 1.0 is random sampling according to the logits. temp >> 1.0 is uniform random sampling. Model is currently untrained so nonsense."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0EEtQLAt_Cw"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PYCia43at-pP"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) #Prevent high losses for banned tokens\n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss * mask\n",
        "  loss = tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels != 0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match * mask) / tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V-zvVDVv1VN"
      },
      "source": [
        "Use a callback to generate text while training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4b5C8M6IvxgJ"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCMSPb0VwQ-q"
      },
      "source": [
        "It generates 3output strings, the first is greedy etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "VHopSRuYwMAY",
        "outputId": "3217bcd7-1238-4017-bfe7-02ce53091cb4"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "can't set attribute 'model'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7dbd602260b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerateText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: can't set attribute 'model'"
          ]
        }
      ],
      "source": [
        "g = GenerateText()\n",
        "g.model = model\n",
        "g.on_epoch_end(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1e455IxwuRc"
      },
      "source": [
        "For more frequent reporting use Dataset.repeat() method and set the steps per epoch and validation steps arguments to same as model.fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "1amMPlV6werw",
        "outputId": "62b5ab6c-e730-44c6-c16b-81f6fa93eff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.7650 - masked_acc: 0.4347\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a surfer is riding a wave\n",
            "recoils surfer on blue beach\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 70ms/step - loss: 2.7650 - masked_acc: 0.4347 - val_loss: 2.9922 - val_masked_acc: 0.3981\n",
            "Epoch 2/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.8180 - masked_acc: 0.4310\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a surfer is riding a wave\n",
            "a man wearing a mud fence down a red surfboard ocean surfing\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - loss: 2.8180 - masked_acc: 0.4310 - val_loss: 2.9844 - val_masked_acc: 0.3927\n",
            "Epoch 3/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.7811 - masked_acc: 0.4354\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a surfer is surfing on a wave\n",
            "a person in a and a white coat puddle\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 2.7810 - masked_acc: 0.4354 - val_loss: 3.0112 - val_masked_acc: 0.3935\n",
            "Epoch 4/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.8051 - masked_acc: 0.4316\n",
            "\n",
            "a surfer in a red wetsuit is riding a wave\n",
            "a man in a red wetsuit is surfing\n",
            "a person is lay down a paddling ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - loss: 2.8055 - masked_acc: 0.4316 - val_loss: 3.0277 - val_masked_acc: 0.3878\n",
            "Epoch 5/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.8116 - masked_acc: 0.4318\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer rides a wave\n",
            "a motorcyclist surfer flips from a wave while covered cliff are fishing on his ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 63ms/step - loss: 2.8109 - masked_acc: 0.4319 - val_loss: 3.0330 - val_masked_acc: 0.3991\n",
            "Epoch 6/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.7942 - masked_acc: 0.4314\n",
            "\n",
            "a surfer in a red wetsuit is surfing a wave\n",
            "a surfer in a red hat is surfing in the wave\n",
            "a beach fishes in the ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 65ms/step - loss: 2.7942 - masked_acc: 0.4314 - val_loss: 2.9502 - val_masked_acc: 0.4023\n",
            "Epoch 7/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.8111 - masked_acc: 0.4259\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer is surfing a wave\n",
            "a it is going down a snowcovered surfboard gives head by another two wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.8108 - masked_acc: 0.4259 - val_loss: 3.0142 - val_masked_acc: 0.3902\n",
            "Epoch 8/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.7690 - masked_acc: 0.4337\n",
            "\n",
            "a man in a red shirt is surfing\n",
            "a person in a red shirt is surfing\n",
            "a golden blonde surfer surfing\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.7687 - masked_acc: 0.4337 - val_loss: 2.9453 - val_masked_acc: 0.3929\n",
            "Epoch 9/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.7742 - masked_acc: 0.4284\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a person in a red wetsuit rides a wave\n",
            "the rapids\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 2.7739 - masked_acc: 0.4285 - val_loss: 2.9626 - val_masked_acc: 0.3924\n",
            "Epoch 10/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.7038 - masked_acc: 0.4393\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer in a white wave in the ocean\n",
            "a man on a geese races on a surfboard\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.7030 - masked_acc: 0.4394 - val_loss: 3.0015 - val_masked_acc: 0.3885\n",
            "Epoch 11/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 2.5896 - masked_acc: 0.4603\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a surfer rides a wave\n",
            "a man and hair pool racing on a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.5897 - masked_acc: 0.4603 - val_loss: 3.0200 - val_masked_acc: 0.3875\n",
            "Epoch 12/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.6462 - masked_acc: 0.4494\n",
            "\n",
            "a surfer in a yellow wetsuit is surfing\n",
            "a wave is riding a wave\n",
            "a surfer with black ski smokes watching a curve\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 2.6456 - masked_acc: 0.4494 - val_loss: 2.9175 - val_masked_acc: 0.3994\n",
            "Epoch 13/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 2.6111 - masked_acc: 0.4508\n",
            "\n",
            "a surfer in a yellow kayak is surfing\n",
            "a person wearing a tan hat is surfing on a wave\n",
            "girl in a red and white surfboard flip blowing on a river\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 70ms/step - loss: 2.6115 - masked_acc: 0.4508 - val_loss: 2.8521 - val_masked_acc: 0.4103\n",
            "Epoch 14/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.5990 - masked_acc: 0.4549\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a man in a red hat is surfing\n",
            "a surfer doing a red raft in a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 58ms/step - loss: 2.5992 - masked_acc: 0.4548 - val_loss: 2.9344 - val_masked_acc: 0.3962\n",
            "Epoch 15/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.5869 - masked_acc: 0.4555\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a surfer in a red wetsuit is surfing\n",
            "a man in a red ocean yellow wetsuit is in a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - loss: 2.5872 - masked_acc: 0.4555 - val_loss: 2.8743 - val_masked_acc: 0.4052\n",
            "Epoch 16/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.5602 - masked_acc: 0.4497\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a man in a red wetsuit is surfing\n",
            "a surfer riding a crashing wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 59ms/step - loss: 2.5605 - masked_acc: 0.4498 - val_loss: 2.8452 - val_masked_acc: 0.4062\n",
            "Epoch 17/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.6434 - masked_acc: 0.4417\n",
            "\n",
            "a man in a red wetsuit is surfing in the ocean\n",
            "a surfer in a red wetsuit is surfing through the ocean\n",
            "a person in a suit and white cap riding a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 71ms/step - loss: 2.6428 - masked_acc: 0.4419 - val_loss: 2.8697 - val_masked_acc: 0.3984\n",
            "Epoch 18/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.5628 - masked_acc: 0.4555\n",
            "\n",
            "a man in a red wetsuit is riding a wave\n",
            "a man in a red wetsuit is surfing in the ocean on a wave\n",
            "a large wave grinds on the surfer wearing a helmet\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 68ms/step - loss: 2.5632 - masked_acc: 0.4555 - val_loss: 2.8368 - val_masked_acc: 0.4013\n",
            "Epoch 19/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.5723 - masked_acc: 0.4535\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a man in a red kayak is in a wave\n",
            "a boy in a red and white wetsuit is sled down a hill\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - loss: 2.5716 - masked_acc: 0.4536 - val_loss: 2.9440 - val_masked_acc: 0.3937\n",
            "Epoch 20/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 2.4289 - masked_acc: 0.4733\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer in a red surfboard riding a wave\n",
            "a surfer rides his neck in a wave in a pool\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 65ms/step - loss: 2.4289 - masked_acc: 0.4733 - val_loss: 2.9490 - val_masked_acc: 0.3976\n",
            "Epoch 21/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.4409 - masked_acc: 0.4732\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer wearing a red kayak is swimming\n",
            "a man riding a surfboard underwater in a river\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.4413 - masked_acc: 0.4732 - val_loss: 2.8857 - val_masked_acc: 0.3942\n",
            "Epoch 22/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 2.4510 - masked_acc: 0.4704\n",
            "\n",
            "a man in a red wetsuit is surfing in the ocean\n",
            "a man in a red boat is riding in a wave\n",
            "a surfer on a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 62ms/step - loss: 2.4511 - masked_acc: 0.4704 - val_loss: 2.9037 - val_masked_acc: 0.3938\n",
            "Epoch 23/100\n",
            "\u001b[1m 99/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 2.4405 - masked_acc: 0.4722\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a man in a red shirt surfing in a wave\n",
            "person wetsuit over water at a canal and eating\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - loss: 2.4406 - masked_acc: 0.4722 - val_loss: 2.8612 - val_masked_acc: 0.4114\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-959253273e70>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m               metrics=[masked_acc])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# If _eval_epoch_iterator exists, delete it after all epochs are done.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_eval_epoch_iterator\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_epoch_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/autotrackable.py\u001b[0m in \u001b[0;36m__delattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     72\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delete_tracking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoTrackable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__delattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_no_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "callbacks = [GenerateText(), tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=masked_loss,\n",
        "              metrics=[masked_acc])\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8aSMU-gw8ar"
      },
      "source": [
        "#Plotting and Predicting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PK24UX0_xH9u"
      },
      "source": [
        "Plot losses and accuracy over the training run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "rExMJggUw_Ol",
        "outputId": "ded24ed0-4230-4f13-91df-ed885eb845fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7a29ffd1b100>"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()\n",
        "\n",
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfZTiPqsxKAk"
      },
      "source": [
        "Now do an attention plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGUBtILHxOFn"
      },
      "outputs": [],
      "source": [
        "result = model.simple_gen(image, temperature=0.0)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29wLZTgKxRb-"
      },
      "source": [
        "Split output back into tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lev_HBLZxTV0"
      },
      "outputs": [],
      "source": [
        "str_tokens = result.split()\n",
        "str_tokens.append('[END]')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUlNe5DsxVcB"
      },
      "source": [
        "The decoder layers each cache the attention scores for the cross attention layer and the shape is (1, Heads, Sequence, Image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_KCqEqGxbjh"
      },
      "outputs": [],
      "source": [
        "attn_maps = [layer.last_attention_scores for layer in model.decoder_layers]\n",
        "[map.shape for map in attn_maps]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT1PJvy1xoN3"
      },
      "source": [
        "So stack the maps along the batch axis then average over the (batch, heads) axes, while splitting the image axis back into height, width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZSlbrOsxv6N"
      },
      "outputs": [],
      "source": [
        "attention_maps = tf.concat(attn_maps, axis=0)\n",
        "attention_maps = einops.reduce(\n",
        "    attention_maps,\n",
        "    'batch heads sequence image -> sequence image',\n",
        "    height=7, width=7,\n",
        "    reduction='mean'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwkCRmRVx7kU"
      },
      "source": [
        "So we have a single attention map for each sequence prediction. The values in each map should sum to 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-jzPiC7yBgB"
      },
      "outputs": [],
      "source": [
        "einops.reduce(attention_maps, 'sequence height width -> sequence', reduction='sum')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F9cvSl2yFy1"
      },
      "source": [
        "So we can see where the model was focussing attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REcU2f_AyJja"
      },
      "outputs": [],
      "source": [
        "def plot_attention_maps(image, str_tokens, attention_map):\n",
        "    fig = plt.figure(figsize=(16, 9))\n",
        "\n",
        "    len_result = len(str_tokens)\n",
        "\n",
        "    titles = []\n",
        "    for i in range(len_result):\n",
        "      map = attention_map[i]\n",
        "      grid_size = max(int(np.ceil(len_result/2)), 2)\n",
        "      ax = fig.add_subplot(3, grid_size, i+1)\n",
        "      titles.append(ax.set_title(str_tokens[i]))\n",
        "      img = ax.imshow(image)\n",
        "      ax.imshow(map, cmap='gray', alpha=0.6, extent=img.get_extent(),\n",
        "                clim=[0.0, np.max(map)])\n",
        "\n",
        "    plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URjAeKUAyLIv"
      },
      "outputs": [],
      "source": [
        "plot_attention_maps(image/255, str_tokens, attention_maps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7Myj0CsyMvw"
      },
      "source": [
        "Lets put this all together and make this function more usable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAGTPBw1yOgt"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def run_and_show_attention(self, image, temperature=0.0):\n",
        "  result_txt = self.simple_gen(image, temperature)\n",
        "  str_tokens = result_txt.split()\n",
        "  str_tokens.append('[END]')\n",
        "\n",
        "  attention_maps = [layer.last_attention_scores for layer in self.decoder_layers]\n",
        "  attention_maps = tf.concat(attention_maps, axis=0)\n",
        "  attention_maps = einops.reduce(\n",
        "      attention_maps,\n",
        "      'batch heads sequence (height width) -> sequence height width',\n",
        "      height=7, width=7,\n",
        "      reduction='mean')\n",
        "\n",
        "  plot_attention_maps(image/255, str_tokens, attention_maps)\n",
        "  t = plt.suptitle(result_txt)\n",
        "  t.set_y(1.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IbHZV6WyV7Q"
      },
      "outputs": [],
      "source": [
        "run_and_show_attention(model, image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViQKVFoLyYxz"
      },
      "source": [
        "We can now try on our own images using our own image path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLJUbdmcyc_e"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/bedroom_hrnet_tutorial.jpg'\n",
        "image_path = tf.keras.utils.get_file(origin=image_url)\n",
        "image = load_image(image_path)\n",
        "\n",
        "run_and_show_attention(model, image)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOSkk17/oAnoYzJq3jPrJNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}